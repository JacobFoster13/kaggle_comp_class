{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac67c756",
   "metadata": {},
   "source": [
    "# Kaggle Competiton Jupyter Script\n",
    "\n",
    "Jacob Foster\n",
    "jdf3438\n",
    "\n",
    "This is my python script for the Business Data Science Kaggle Competition. I used a variety of preprocessing tactics and predictive models to place 3rd on the private leaderboard for the class.\n",
    "\n",
    "#### Table of Contents\n",
    "* Data Exploration and Preprocessing\n",
    "* Model Creation\n",
    "    * Basic Models\n",
    "    * Boosting Models\n",
    "    * Neural Network\n",
    "* Initial Predictions and Evaluation\n",
    "* Model Improvement\n",
    "    * Feature Selection\n",
    "    * Ensembling\n",
    "    * Personal Stacking\n",
    "* Final Model Training\n",
    "* Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01bc88",
   "metadata": {},
   "source": [
    "In addition to using XGBoost, I research some other libraries that provided powerful classification models. Ultimately, I decided to use CatBoost and LightGBM after reading about some algorithms previous Kaggle winners had used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd5056",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# installs\n",
    "! pip install xgboost\n",
    "! pip install catboost\n",
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33fb831",
   "metadata": {},
   "source": [
    "I installed some basic data manipulation and visualization libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcc9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1949a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv(\"train.csv\", index_col='Id')\n",
    "test = pd.read_csv(\"test.csv\", index_col='Id')\n",
    "rf1 = pd.read_csv(\"rf1.csv\", index_col='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab17365",
   "metadata": {},
   "source": [
    "## Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c8d78",
   "metadata": {},
   "source": [
    "During my data exploration, I did some basic description of the data to learn some of the key statistical measurements were, such as minimum, maximum, average, standard deviation, etc. I found that null values were denoted with a -999 in place of the missing value. I removed these values to increase the integrity and representativeness of the data. After looking through the columns of data, I eventually decided to remove columns that only contained the number zero to test if that would improve the predictive capabilities of the models. This greatly improved the predictions on the public leaderboard so I kept the changes. Instead of doing this manually for each column, I did it programmatically, removing columns where the maximum and minimum values were both zero.\n",
    "\n",
    "I also created a separate dataframe `X_norm` where I normalized the data to prevent the numeric columns with comparatively large numbers from skewing the predictions of the models. Ultimately, this did not help improve any of my predictive accuracy either locally or on Kaggle. Next, I created a function that would remove records that contained outliers. For the purpose of this dataset, I considered anything that was greater than three standard deviations from the mean as an outlier. Removing these outliers left me with too few rows to remain representative of the data (the final number of rows was ~1120). Regardless, I still tested my models locally to determine if there was any significant benefit to removing these outliers. Upon running the tests, the accuracy for all my models suffered greatly and I decided that removing the outliers was no longer a good idea. This was relatively surprising to me at first since most dataset I had previously worked with required the removal of outliers to improve the quality of data, but after looking closer at the numeric columns of the data, the range of the values was so large it made sense that so many values were being removed.\n",
    "\n",
    "I then created a new dataframe `interaction` that contained a multiplicative interaction term between every term of the data. This dataframe was to be used later for forward and backward selection to determine if there was any predictive power hidden in an interaction term within the data. As we will see later, the interaction terms and forward/backward selection themselves did not offer much benefit to the model.\n",
    "\n",
    "Ultimately, the preprocessing operations that yielded the most predictive data were removing the noisy columns of only zeros and removing the rows that had missing (-999) values. Thus, the preprocessing was relatively simple, yet effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b03d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "      <td>2853.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.438486</td>\n",
       "      <td>-0.332282</td>\n",
       "      <td>-2.367683</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.445846</td>\n",
       "      <td>0.031195</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.041360</td>\n",
       "      <td>0.035401</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.191377</td>\n",
       "      <td>0.079215</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.039958</td>\n",
       "      <td>1.055731</td>\n",
       "      <td>4206.567473</td>\n",
       "      <td>0.042411</td>\n",
       "      <td>0.229861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496289</td>\n",
       "      <td>18.703948</td>\n",
       "      <td>49.436698</td>\n",
       "      <td>0.081349</td>\n",
       "      <td>0.497146</td>\n",
       "      <td>0.173875</td>\n",
       "      <td>0.049481</td>\n",
       "      <td>0.199156</td>\n",
       "      <td>0.184824</td>\n",
       "      <td>0.105330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139952</td>\n",
       "      <td>0.393454</td>\n",
       "      <td>0.270121</td>\n",
       "      <td>0.069891</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.195895</td>\n",
       "      <td>0.312289</td>\n",
       "      <td>2429.577650</td>\n",
       "      <td>0.271268</td>\n",
       "      <td>1.892760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.487258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2114.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.033209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4197.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6302.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.545231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8416.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.473742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Y            2            3            4            5  \\\n",
       "count  2853.000000  2853.000000  2853.000000  2853.000000  2853.000000   \n",
       "mean      0.438486    -0.332282    -2.367683     0.006660     0.445846   \n",
       "std       0.496289    18.703948    49.436698     0.081349     0.497146   \n",
       "min       0.000000  -999.000000  -999.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                 6            7            8            9           10  ...  \\\n",
       "count  2853.000000  2853.000000  2853.000000  2853.000000  2853.000000  ...   \n",
       "mean      0.031195     0.002454     0.041360     0.035401     0.011216  ...   \n",
       "std       0.173875     0.049481     0.199156     0.184824     0.105330  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "                77           78           79           80           81  \\\n",
       "count  2853.000000  2853.000000  2853.000000  2853.000000  2853.000000   \n",
       "mean      0.019979     0.191377     0.079215     0.004907     0.000351   \n",
       "std       0.139952     0.393454     0.270121     0.069891     0.018722   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                82           83           84           85           86  \n",
       "count  2853.000000  2853.000000  2853.000000  2853.000000  2853.000000  \n",
       "mean      0.039958     1.055731  4206.567473     0.042411     0.229861  \n",
       "std       0.195895     0.312289  2429.577650     0.271268     1.892760  \n",
       "min       0.000000     1.000000    -3.000000     0.000000    -8.487258  \n",
       "25%       0.000000     1.000000  2114.000000     0.000000    -1.033209  \n",
       "50%       0.000000     1.000000  4197.000000     0.000000     0.516411  \n",
       "75%       0.000000     1.000000  6302.000000     0.000000     1.545231  \n",
       "max       1.000000     6.000000  8416.000000     7.000000     7.473742  \n",
       "\n",
       "[8 rows x 86 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e60e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find that -999 likely represents missing values\n",
    "train[\"3\"].value_counts()\n",
    "\n",
    "# drop these missing values\n",
    "train.replace(-999, np.nan, inplace=True)\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86422ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noisy columns with only 0s\n",
    "noise = [col for col in train.columns if (train[col].min() == 0) and (train[col].max() == 0)]\n",
    "train = train.drop(noise, axis=1)\n",
    "test = test.drop(noise, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba693ae7",
   "metadata": {},
   "source": [
    "The data does not need a `train_test_split` since the entire dataset will be used later during the accuracy score phase using cross-validation to evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07cdf68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for testing algos and feature generation\n",
    "X = train.drop(columns=\"Y\")\n",
    "y = train['Y']\n",
    "\n",
    "# split the data and set the random state seed\n",
    "rs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82c5ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    if s.max() > 0:\n",
    "        return (s - s.min())/(s.max() - s.min())\n",
    "    return 0.0\n",
    "normalized = X.copy()\n",
    "X_norm = normalized.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e512ff41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1126"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe with all outliers removed\n",
    "z_scores = np.abs((train - train.mean()) / train.std())\n",
    "no_outliers = train[(z_scores < 3).all(axis=1)]\n",
    "len(no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8bd3f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>86x76</th>\n",
       "      <th>86x77</th>\n",
       "      <th>86x78</th>\n",
       "      <th>86x79</th>\n",
       "      <th>86x80</th>\n",
       "      <th>86x81</th>\n",
       "      <th>86x82</th>\n",
       "      <th>86x83</th>\n",
       "      <th>86x84</th>\n",
       "      <th>86x85</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7587.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10700.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.785554</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-3.785554</td>\n",
       "      <td>-7858.809122</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-1.733455</td>\n",
       "      <td>-4335.370892</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-1.869119</td>\n",
       "      <td>-4764.384203</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7056 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Y    2    3  4  5  6  7  8  9  10  ...     86x76  86x77  86x78  86x79  \\\n",
       "Id                                     ...                                  \n",
       "1   0  0.0  0.0  0  1  0  0  0  0   0  ...  1.000000    0.0    0.0    0.0   \n",
       "2   1  0.0  0.0  0  1  0  0  0  0   0  ...  2.000000    0.0    0.0    0.0   \n",
       "3   1  0.0  0.0  0  1  0  0  0  1   1  ... -3.785554   -0.0   -0.0   -0.0   \n",
       "4   0  0.0  1.0  0  0  0  0  0  0   0  ... -0.000000   -0.0   -0.0   -0.0   \n",
       "5   0  0.0  0.0  0  0  0  0  0  0   0  ... -0.000000   -0.0   -0.0   -0.0   \n",
       "\n",
       "    86x80  86x81  86x82     86x83         86x84  86x85  \n",
       "Id                                                      \n",
       "1     0.0    0.0    0.0  1.000000   7587.000000    0.0  \n",
       "2     0.0    0.0    0.0  2.000000  10700.000000    0.0  \n",
       "3    -0.0   -0.0   -0.0 -3.785554  -7858.809122   -0.0  \n",
       "4    -0.0   -0.0   -0.0 -1.733455  -4335.370892   -0.0  \n",
       "5    -0.0   -0.0   -0.0 -1.869119  -4764.384203   -0.0  \n",
       "\n",
       "[5 rows x 7056 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate interaction terms\n",
    "interaction = train.copy()\n",
    "for i in train.columns:\n",
    "    for j in train.columns:\n",
    "        if i != j:\n",
    "            interaction[i+'x'+j] = interaction[i]*interaction[j]\n",
    "interaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e1aea",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c0a08",
   "metadata": {},
   "source": [
    "In order to create the most predictive model possible, I started with simpler models then moved to more complex models that used boosting and neural networks. After researching the models, reading documentation, and reading advice from former Kaggle achievers on several forums, I decided the parameters I wanted to target using the GridSearchCV and picked a range of reasonable values to test for my models. \n",
    "\n",
    "The Basic Models required very little time to train and had less paramters to test different values for. Out of the Basic Models, Random Forest was the best single predictor.\n",
    "\n",
    "The Boosting Models required more time than the Basic Models to train and research. After consulting online forums I decided to include the LightGBM classifier since it had a strong reputation of working well for other Kaggle classification competitions. Since there were so many parameters for the CatBoost, XGBoost, and LightGBM, I had to research which parameters historically had the most impact on the predictive capabilities of the models. Something that was really surprising to me was the stark difference in the parameters that were considered the \"best\" for a model depending on the preprocessing performed on the data. The parameters for the models without the noisy data removed or when the data was normalized were drastically different from each other. \n",
    "\n",
    "After some time researching and experimenting, I determined these were the most powerful predictors for CatBoost and LightGBM:\n",
    "* CatBoost\n",
    "    * `depth`\n",
    "    * `learning_rate`\n",
    "    * `l2_leaf_reg`\n",
    "    * `iterations`\n",
    "    * `random_strength`\n",
    "    \n",
    "* LightGBM\n",
    "    * `objective`\n",
    "    * `metric`\n",
    "    * `boosting_type`\n",
    "    * `max_depth`\n",
    "    * `num_iterations`\n",
    "    * `learning_rate`\n",
    "    \n",
    "After running the GridSearchCV on most of the Boosting Models, I used the returned `best_params_` as the new parameters for each model. The best parameters for each model and visible in their respective cells. The LightGBM model was the best single predictor out of the Boosting Models.\n",
    "\n",
    "Next, I trained a Multilayer Perceptron to test the predictive capabilities of a relatively simple neural network. I was disappointed at the lack of accuracy of the Multilayer Perceptron, and since the model took about 15 hours to perform GridSearchCV on, I opted against training another Neural Network Model for the sake of time.\n",
    "\n",
    "In total, I tried 8 models and a combined total of 11,166 models through the use of GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5096bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from catboost import CatBoostClassifier\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04203d6c",
   "metadata": {},
   "source": [
    "### Basic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccebe1",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "398e5836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(random_state=rs, max_iter=1000).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed73172",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d660b119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_depth=7, n_estimators=50,\n",
       "                       random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_depth=7, n_estimators=50,\n",
       "                       random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=7, n_estimators=50,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100, 200, 300],\n",
    "    \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "    \"max_depth\": [3, 4, 5, 6, 7],\n",
    "}\n",
    "\n",
    "grid_rf_model = RandomForestClassifier(random_state=rs, criterion='entropy', n_estimators=50, max_depth=7)\n",
    "\n",
    "'''\n",
    "grid_rf_model = GridSearchCV(estimator=grid_rf_model, param_grid=rf_params, cv=5, n_jobs=-1)\n",
    "best: {criterion='entropy', n_estimators=50, max_depth=7}\n",
    "'''\n",
    "\n",
    "grid_rf_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae6575",
   "metadata": {},
   "source": [
    "### Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c890eb",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d0d8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_model = AdaBoostClassifier(random_state=rs, \n",
    "                               n_estimators=100, \n",
    "                               learning_rate=.2).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da26a75",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdf14ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.02, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.02, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.02, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=0, ...)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.075, 0.1, .2],\n",
    "    'max_depth': [3, 4, 5, 6, 7]\n",
    "}\n",
    "grid_xgb_model = xgb.XGBClassifier(random_state=rs,\n",
    "                                   n_estimators=300, \n",
    "                                   learning_rate=.02, \n",
    "                                   max_depth=3,\n",
    "                                   booster='gbtree')\n",
    "\n",
    "'''\n",
    "# grid_xgb_model = GridSearchCV(estimator=grid_xgb_model, param_grid=xgb_params, cv=5, n_jobs=-1)\n",
    "# new best:  {'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 300}\n",
    "'''\n",
    "grid_xgb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c26f39",
   "metadata": {},
   "source": [
    "#### GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e55e54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(loss=&#x27;exponential&#x27;, max_depth=6, n_estimators=50,\n",
       "                           random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(loss=&#x27;exponential&#x27;, max_depth=6, n_estimators=50,\n",
       "                           random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(loss='exponential', max_depth=6, n_estimators=50,\n",
       "                           random_state=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_params = {'n_estimators': [50, 100, 200, 300, 500],\n",
    "               'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.1, 0.15,],\n",
    "               'max_depth': [3, 4, 5, 6, 7],\n",
    "               'loss': ['log_loss', 'exponential']}\n",
    "grid_gbc_model = GradientBoostingClassifier(learning_rate=0.1, \n",
    "                                            loss='exponential', \n",
    "                                            max_depth=6, \n",
    "                                            n_estimators=50, \n",
    "                                            random_state=rs)\n",
    "'''\n",
    "grid_gbc_model = GridSearchCV(estimator=grid_gbc_model, param_grid=gbc_params, cv=5, n_jobs=-1)\n",
    "new best: {'learning_rate': 0.1, 'loss': 'exponential', 'max_depth': 6, 'n_estimators': 50}\n",
    "'''\n",
    "\n",
    "grid_gbc_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8f9bb",
   "metadata": {},
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7e9759e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1ee5b9f6490>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_parameters = {'depth': [4, 5, 6, 7, 8],\n",
    "                  'learning_rate' : [0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1],\n",
    "                  'iterations'    : [50, 100, 200, 300],\n",
    "                  'l2_leaf_reg'   : [3, 4, 5, 6, 7, 8],\n",
    "                  'random_strength': [3, 4, 5, 6, 7, 8]}\n",
    "\n",
    "grid_cat_model = CatBoostClassifier(random_state=rs, \n",
    "                                    verbose=False, \n",
    "                                    depth=5, \n",
    "                                    l2_leaf_reg=7,\n",
    "                                    iterations=200,\n",
    "                                    learning_rate=0.09,\n",
    "                                    random_strength=7,\n",
    "                                    loss_function='CrossEntropy',\n",
    "                                    eval_metric='AUC')\n",
    "'''\n",
    "grid_cat_model = GridSearchCV(estimator=grid_cat_model, param_grid = cat_parameters, cv = 5, n_jobs=-1)\n",
    "new best \n",
    " {'depth': 5, 'iterations': 200, 'l2_leaf_reg': 7, 'learning_rate': 0.09, 'random_strength': 7}\n",
    "'''\n",
    "grid_cat_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8266b",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "776c879d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.05, max_depth=4, metric=&#x27;binary_logloss&#x27;,\n",
       "               num_iterations=200, objective=&#x27;binary&#x27;, random_state=0,\n",
       "               verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.05, max_depth=4, metric=&#x27;binary_logloss&#x27;,\n",
       "               num_iterations=200, objective=&#x27;binary&#x27;, random_state=0,\n",
       "               verbose=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(learning_rate=0.05, max_depth=4, metric='binary_logloss',\n",
       "               num_iterations=200, objective='binary', random_state=0,\n",
       "               verbose=-1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'objective': ['binary', 'regression'],\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': ['gbdt', 'rf', 'dart'],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'num_iterations': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.025, 0.05, 1],\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "grid_lgb_model = lgb.LGBMClassifier(random_state=rs, \n",
    "                                    boosting_type='gbdt',\n",
    "                                    learning_rate=.05,\n",
    "                                    max_depth=4,\n",
    "                                    metric='binary_logloss',\n",
    "                                    num_iterations=200,\n",
    "                                    objective='binary',\n",
    "                                    verbose=-1)\n",
    "\n",
    "'''\n",
    "grid_lgb_model = GridSearchCV(estimator=grid_lgb_model, param_grid=lgb_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "best: {'boosting_type': 'gbdt', 'learning_rate': 0.05, 'max_depth': 4, \n",
    "'metric': 'binary_logloss', 'num_iterations': 200, 'objective': 'binary'}\n",
    "'''\n",
    "grid_lgb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278730d",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a5722",
   "metadata": {},
   "source": [
    "#### MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1892bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.00015, hidden_layer_sizes=(100, 50),\n",
       "              max_iter=600, random_state=0, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, alpha=0.00015, hidden_layer_sizes=(100, 50),\n",
       "              max_iter=600, random_state=0, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.00015, hidden_layer_sizes=(100, 50),\n",
       "              max_iter=600, random_state=0, solver='lbfgs')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_params = {'hidden_layer_sizes': [(100, 50), (150, 100, 50), (50,), (100,), (50, 100, 50), (150, 100)],\n",
    "              'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "              'solver': ['adam', 'sgd', 'lbfgs'],\n",
    "              'max_iter': [100, 200, 300, 400],\n",
    "              'alpha': [0.00005, 0.0001, 0.00015, 0.00025, 0.0005, 0.001],\n",
    "              'learning_rate': ['constant', 'invscaling', 'adaptive']}\n",
    "\n",
    "grid_mlp_model = MLPClassifier(activation='tanh', \n",
    "                               alpha=0.00015, \n",
    "                               hidden_layer_sizes=(100, 50), \n",
    "                               learning_rate='constant', \n",
    "                               max_iter=600, \n",
    "                               solver='lbfgs', \n",
    "                               random_state=rs)\n",
    "'''\n",
    "grid_mlp_model = GridSearchCV(estimator=grid_mlp_model, param_grid=mlp_params, cv=5, n_jobs=-1)\n",
    "best parameters were activation='tanh', alpha=0.00015, hidden_layer_sizes=(100, 50), \n",
    "learning_rate='constant', max_iter=400, solver='lbfgs'\n",
    "'''\n",
    "grid_mlp_model.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ca9ed",
   "metadata": {},
   "source": [
    "## Initial Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d35c70",
   "metadata": {},
   "source": [
    "To evaluate the predictive capability of each model, I used `cross_val_predict` from Sci-kit Learn so each model could be trained and evaluated on the entireity of the training dataset. Using the `predict_proba` method, I generated soft prediction values for the predicted class. I then used the AUC of each model to evaluate the accuracy of the predictions since that was the metric used to evalute our performance on Kaggle. Using the cross validation prediction method worked extremely well as it gave me a relatively accurate benchmark of what to expect from my Kaggle results. For the sake of time, I used 5-fold validation as using a higher number of folds took much more time and did not offer any signifcant insight to the predictive power of the data.\n",
    "\n",
    "At the end of each `cross_val_predict` statement there is an index of `[:, 1]`. This is used because the `predict_proba` method produces a 2-dimensional list with a length of `X` and a width of 2. The second index of each sublist is the predicted probability that a record is of class 1. Thus, the index of `[:, 1]` gathers the entire vector of the predicted probabilities that each record is of class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "539d540b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic AUC: 0.8759196190751259\n",
      "Random Forest AUC: 0.8869991736465455\n",
      "AdaBoost AUC: 0.8895943146080036\n",
      "GradientBoosting: 0.899469689676198\n",
      "XGBoost: 0.8995047896408974\n",
      "CatBoost: 0.9027615649369405\n",
      "LGBM:\t 0.9035605905619203\n",
      "MLP: 0.8734310315779339\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "log_pred = cross_val_predict(log_model, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "rf_pred = cross_val_predict(grid_rf_model, X, y, cv=5, method='predict_proba')[:,1]\n",
    "adb_pred = cross_val_predict(adb_model, X, y, cv=5, method='predict_proba')[:,1]\n",
    "gbc_pred = cross_val_predict(grid_gbc_model, X, y, cv=5, method='predict_proba')[:,1]\n",
    "xgb_pred = cross_val_predict(grid_xgb_model, X, y, cv=5, method='predict_proba')[:,1]\n",
    "cat_pred = cross_val_predict(grid_cat_model, X, y, cv=5, method='predict_proba')[:,1]\n",
    "lgb_pred = cross_val_predict(grid_lgb_model, X, y, cv=5, method='predict_proba')[:,1]\n",
    "mlp_pred = cross_val_predict(grid_mlp_model, X, y, cv=5, method='predict_proba')[:,1]\n",
    "\n",
    "# get auc score\n",
    "log_auc = roc_auc_score(y, log_pred)\n",
    "rf_auc = roc_auc_score(y, rf_pred)\n",
    "adb_auc = roc_auc_score(y, adb_pred)\n",
    "gbc_auc = roc_auc_score(y, gbc_pred)\n",
    "xgb_auc = roc_auc_score(y, xgb_pred)\n",
    "cat_auc = roc_auc_score(y, cat_pred)\n",
    "lgb_auc = roc_auc_score(y, lgb_pred)\n",
    "mlp_auc = roc_auc_score(y, mlp_pred)\n",
    "\n",
    "# display scores\n",
    "print(f\"Logistic AUC: {log_auc}\")\n",
    "print(f\"Random Forest AUC: {rf_auc}\")\n",
    "print(f\"AdaBoost AUC: {adb_auc}\")\n",
    "print(f\"GradientBoosting: {gbc_auc}\")\n",
    "print(f\"XGBoost: {xgb_auc}\")\n",
    "print(f\"CatBoost: {cat_auc}\")\n",
    "print(f\"LGBM:\\t {lgb_auc}\")\n",
    "print(f\"MLP: {mlp_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840e39f",
   "metadata": {},
   "source": [
    "## Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3b53a",
   "metadata": {},
   "source": [
    "After getting the initial model predictions and performance, I used several of the ideas discussed in class in an attempt to improve the predictive capability of my models. I attempted backward and forward selection, model stacking, and bagging various models.\n",
    "\n",
    "Backward selection was extremely time intensive and did not offer any significant improvement when used on the regular dataset or the dataset created with the interaction terms. It took several hours to run and did not perform as well as the basic boosting models did. Thus, when given the time sacrifice for the high computational cost, it was not beneficial to use backward selection.\n",
    "\n",
    "Surprisingly, forward selection was a very similar story to backward selection. The computation was time intensive and ultimately did not produce as predictive of a model as the base boosting models. For both backward and forward selection, I added LightGBM after I has run the initial tests of feature selection, and since the performance for the original 4 boosted models was not improved, I decided it was not worth the time sacrifice to test backward or forward selection on LightGBM.\n",
    "\n",
    "Next, I used the `StackingClassifier` from Sci-kit Learn, combining the AdaBoost, GradientBoost, XGBoost, and CatBoost models into a singular model. Again, this model was surprisingly not as powerfully predictive as the basic, unstacked models. When creating the stacked model, I was confident that the results would improve, but I was wrong; it was still better to go with a singular boosted model over the stacked model, even when considering the performance on the privat leaderboard. I also bagged the stacked model, but this did not prove to be helpful either. Admittedly, I could have used GridSearchCV to explore what the best `final_estimator` should have been for my stacking model, but the results on Kaggle were too discouraging to continue exploring this idea, so I decided to pursue other ideas that seemed more predictive. \n",
    "\n",
    "Even though stacking using the `StackingClassifier` did not perform as well as I was hoping, I tried to create my own stacked model by combining the predictions of the boosted models in a single feature of the dataset. This underperformed in both my local environment and on Kaggle **but** it did lead me to the idea that ultimately delivered my best predictions on Kaggle; thus, securing me 3rd place on the private leaderboard. Even though my idea of combining predictions into a singular feature did not work, it was a crucial stepping stone to the idea that did work.\n",
    "\n",
    "Lastly, I bagged all of the boosted models in an attempt to create more generalized models that would perform better on data they had not seen before. The CatBoost bagged model performed very well on Kaggle and was my leading score for while before trying my final idea. The bagging was computationally intense, but I think the time sacrifice was worth the predictive capabilities of the final models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa0ecb",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65eca09",
   "metadata": {},
   "source": [
    "#### Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ac5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward select the models\n",
    "adb_backward = RFECV(estimator=adb_model)\n",
    "gbc_backward = RFECV(estimator=grid_gbc_model)\n",
    "xgb_backward = RFECV(estimator=grid_xgb_model)\n",
    "cat_backward = RFECV(estimator=grid_cat_model)\n",
    "\n",
    "# fit the models\n",
    "adb_backward.fit(X, y)\n",
    "gbc_backward.fit(X, y)\n",
    "xgb_backward.fit(X, y)\n",
    "cat_backward.fit(X, y)\n",
    "\n",
    "# adjust X based on the features selected for each model\n",
    "adb_selected = X[:, adb_backward.support_]\n",
    "gbc_selected = X[:, gbc_backward.support_]\n",
    "xgb_selected = X[:, xgb_backward.support_]\n",
    "cat_selected = X[:, cat_backward.support_]\n",
    "\n",
    "# fit the adjusted models\n",
    "adb_model.fit(adb_selected, y)\n",
    "grid_gbc_model.fit(gbc_selected, y)\n",
    "grid_xgb_model.fit(xgb_selected, y)\n",
    "grid_cat_model.fit(cat_selected, y)\n",
    "\n",
    "# generate predictions from the models\n",
    "adb_backward_pred = cross_val_predict(grid_mlp_model, adb_selected, y, cv=5, method='predict_proba')[:,1]\n",
    "gbc_backward_pred = cross_val_predict(grid_mlp_model, gbc_selected, y, cv=5, method='predict_proba')[:,1]\n",
    "xgb_backward_pred = cross_val_predict(grid_mlp_model, xgb_selected, y, cv=5, method='predict_proba')[:,1]\n",
    "cat_backward_pred = cross_val_predict(grid_mlp_model, cat_selected, y, cv=5, method='predict_proba')[:,1]\n",
    "\n",
    "# get the AUC score for each model\n",
    "adb_backward_auc = roc_auc_score(y, adb_backward_pred)\n",
    "gbc_backward_auc = roc_auc_score(y, gbc_backward_pred)\n",
    "xgb_backward_auc = roc_auc_score(y, xgb_backward_pred)\n",
    "cat_backward_auc = roc_auc_score(y, cat_backward_pred)\n",
    "\n",
    "# dipslay the AUCs\n",
    "print(f\"adb_backward AUC: {adb_backward_auc}\")\n",
    "print(f\"gbc_backward AUC: {gbc_backward_auc}\")\n",
    "print(f\"xgb_backward AUC: {xgb_backward_auc}\")\n",
    "print(f\"cat_backward AUC: {cat_backward_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4de72a",
   "metadata": {},
   "source": [
    "#### Forward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037c03b",
   "metadata": {},
   "source": [
    "When doing forward selection, I selected the CatBoost and GradientBoosting models as my initital test models since they routinely achieved a high AUC on their base models. After conducting forward selection and testing these models, I decided to not continue with forward selection for the other models since forward selection did not result in an increased in predictive power for these models. Initially, I chose 9 features as the `n_features_to_select` to follow the same philosophy as random forest feature selection, but this resulted in a poor local AUC. Thus, I increased the `n_features_to_select` to 15 for a compromise between training time and increased predictiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector as sfs\n",
    "\n",
    "# build the forward selection models\n",
    "gbc_sfs = sfs(estimator=grid_gbc_model, n_features_to_select=15, direction='forward', cv=5, n_jobs=-1)\n",
    "cat_sfs = sfs(estimator=grid_cat_model, n_features_to_select=15, direction='forward', cv=5, n_jobs=-1)\n",
    "\n",
    "# fit the models\n",
    "gbc_sfs.fit(X, y)\n",
    "cat_sfs.fit(X, y)\n",
    "\n",
    "# generate predictions\n",
    "cat_sfs_pred = cross_val_predict(cat_sfs, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "cat_sfs_pred = cross_val_predict(cat_sfs, X, y, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# get AUC\n",
    "gbc_sfs_auc = roc_auc_score(y, gbc_sfs_pred)\n",
    "cat_sfs_auc = roc_auc_score(y, cat_sfs_pred)\n",
    "\n",
    "# display AUCs\n",
    "print(f\"gbc_sfs AUC: {gbc_sfs_auc}\")\n",
    "print(f\"cat_sfs AUC: {cat_sfs_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966c07e",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b2f46",
   "metadata": {},
   "source": [
    "#### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9887dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model AUC: 0.8976555229293027\n",
      "Stacked Bagged Model AUC: 0.897493060235551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# estimators to stack\n",
    "base_estimators = [\n",
    "    ('adaboost', adb_model),\n",
    "    ('gradientboost', grid_gbc_model),\n",
    "    ('xgboost', grid_xgb_model),\n",
    "    ('catboost', grid_cat_model)\n",
    "]\n",
    "\n",
    "# create the stacked model\n",
    "stacked_model = StackingClassifier(estimators=base_estimators, \n",
    "                                   stack_method='predict_proba',\n",
    "                                   final_estimator=xgb.XGBClassifier(random_state=rs, n_estimator=100, learning_rate=.02), \n",
    "                                   passthrough=True, \n",
    "                                   n_jobs=-1)\n",
    "# bag the stacked model\n",
    "stacked_bag = BaggingClassifier(estimator=stacked_model, n_estimators=20, n_jobs=-1)\n",
    "\n",
    "# fit the models\n",
    "stacked_model.fit(X, y)\n",
    "stacked_bag.fit(X, y)\n",
    "\n",
    "# generate predictions\n",
    "stack_pred = cross_val_predict(stacked_model, X, y, cv=5, method='predict_proba')[:,1]\n",
    "stack_bag_pred = cross_val_predict(stacked_bag, X, y, cv=5, method='predict_proba')[:,1]\n",
    "\n",
    "# get AUC of predictions\n",
    "stack_auc = roc_auc_score(y, stack_pred)\n",
    "stack_bag_auc = roc_auc_score(y, stack_bag_pred)\n",
    "\n",
    "# display the AUCs\n",
    "print(f\"Stacked Model AUC: {stack_auc}\")\n",
    "print(f\"Stacked Bagged Model AUC: {stack_bag_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf61c7",
   "metadata": {},
   "source": [
    "This next cell was the experiment that ultimately led me to the idea that would deliver my best Kaggle performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "374bc1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (Stacked): 0.8948312293411637\n",
      "CatBoost (Stacked): 0.8934279828952858\n",
      "GBoost (Stacked): 0.8952105596739512\n"
     ]
    }
   ],
   "source": [
    "# stacking retrain on same model\n",
    "stacked = X.copy()\n",
    "stacked['combo_pred'] = ((cat_pred + xgb_pred + gbc_pred + adb_pred) / 4)\n",
    "\n",
    "grid_xgb_model.fit(stacked, y)\n",
    "grid_cat_model.fit(stacked, y)\n",
    "grid_gbc_model.fit(stacked, y)\n",
    "\n",
    "xgb_stack_pred = cross_val_predict(grid_xgb_model, stacked, y, cv=5, method='predict_proba')[:,1]\n",
    "cat_stack_pred = cross_val_predict(grid_cat_model, stacked, y, cv=5, method='predict_proba')[:,1]\n",
    "gbc_stack_pred = cross_val_predict(grid_gbc_model, stacked, y, cv=5, method='predict_proba')[:,1]\n",
    "\n",
    "xgb_stack_auc = roc_auc_score(y, xgb_stack_pred)\n",
    "cat_stack_auc = roc_auc_score(y, cat_stack_pred)\n",
    "gbc_stack_auc = roc_auc_score(y, gbc_stack_pred)\n",
    "\n",
    "print(f\"XGBoost (Stacked): {xgb_stack_auc}\")\n",
    "print(f\"CatBoost (Stacked): {cat_stack_auc}\")\n",
    "print(f\"GBoost (Stacked): {gbc_stack_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a8b89",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e867c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost AUC: 0.8892756570713392\n",
      "GradientBoosting: 0.8984778649594043\n",
      "XGBoost: 0.8986187662462695\n",
      "CatBoost: 0.9017531930939315\n",
      "LightGBM: 0.9036716568787908\n",
      "RandomForest: 0.8880245940438368\n"
     ]
    }
   ],
   "source": [
    "# bagging only the best performing models\n",
    "adb_bag = BaggingClassifier(estimator=adb_model, n_estimators=20, n_jobs=-1)\n",
    "gbc_bag = BaggingClassifier(estimator=grid_gbc_model, n_estimators=20, n_jobs=-1)\n",
    "xgb_bag = BaggingClassifier(estimator=grid_xgb_model, n_estimators=20, n_jobs=-1)\n",
    "cat_bag = BaggingClassifier(estimator=grid_cat_model, n_estimators=20, n_jobs=-1)\n",
    "lgb_bag = BaggingClassifier(estimator=grid_lgb_model, n_estimators=20, n_jobs=-1)\n",
    "rf_bag = BaggingClassifier(estimator=grid_rf_model, n_estimators=20, n_jobs=-1)\n",
    "\n",
    "# fitting the models\n",
    "adb_bag.fit(X, y)\n",
    "gbc_bag.fit(X, y)\n",
    "xgb_bag.fit(X, y)\n",
    "cat_bag.fit(X, y)\n",
    "lgb_bag.fit(X, y)\n",
    "rf_bag.fit(X, y)\n",
    "\n",
    "# get the model predictions\n",
    "adb_bag_pred = cross_val_predict(adb_bag, X, y, cv=5, method='predict_proba')[:,1]\n",
    "gbc_bag_pred = cross_val_predict(gbc_bag, X, y, cv=5, method='predict_proba')[:,1]\n",
    "xgb_bag_pred = cross_val_predict(xgb_bag, X, y, cv=5, method='predict_proba')[:,1]\n",
    "cat_bag_pred = cross_val_predict(cat_bag, X, y, cv=5, method='predict_proba')[:,1]\n",
    "lgb_bag_pred = cross_val_predict(lgb_bag, X, y, cv=5, method='predict_proba')[:,1]\n",
    "rf_bag_pred = cross_val_predict(rf_bag, X, y, cv=5, method='predict_proba')[:,1]\n",
    "\n",
    "# get the AUC score\n",
    "adb_bag_auc = roc_auc_score(y, adb_bag_pred)\n",
    "gbc_bag_auc = roc_auc_score(y, gbc_bag_pred)\n",
    "xgb_bag_auc = roc_auc_score(y, xgb_bag_pred)\n",
    "cat_bag_auc = roc_auc_score(y, cat_bag_pred)\n",
    "lgb_bag_auc = roc_auc_score(y, lgb_bag_pred)\n",
    "rf_bag_auc = roc_auc_score(y, rf_bag_pred)\n",
    "\n",
    "# display AUC scores\n",
    "print(f\"AdaBoost AUC: {adb_bag_auc}\")\n",
    "print(f\"GradientBoosting: {gbc_bag_auc}\")\n",
    "print(f\"XGBoost: {xgb_bag_auc}\")\n",
    "print(f\"CatBoost: {cat_bag_auc}\")\n",
    "print(f\"LightGBM: {lgb_bag_auc}\")\n",
    "print(f\"RandomForest: {rf_bag_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643032da",
   "metadata": {},
   "source": [
    "### Personal Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d92591",
   "metadata": {},
   "source": [
    "This was the idea that generate the best predictions on Kaggle. I got this idea from my previous idea to create a new feature based on the stacked predictions from the various models. When that posed some issue with being able to fit the `test` dataset, I decided to just average the predictions from the test set to create a `y_pred` based on the average soft prediction value of the models I chose to combine.\n",
    "\n",
    "The AUC from the combined `predict_probas` was notably high, so I was unsure of what the results would be on Kaggle since I was concerned the predictions would be overfit to the provided training data. However, when I tested the results on Kaggle, the score was lower than the local AUC, but it provided the most accurate Kaggle predictions I was able to generate. I was surprised that the predictions remained relatively accurate even after appearing to be very overfit to the initial data and even more surprised when the accuracy translated to the private leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d547a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_lgb AUC: 0.9508720836943616\n",
      "cat_gbc_xgb_adb AUC: 0.9552279893135649\n",
      "cat_lgb_gbc_xgb AUC: 0.9554160248387407\n",
      "cat_lgb_gbc_xgb_adb AUC: 0.9549010582137928\n",
      "cat_xgb_lgb_gbc_rf AUC: 0.9523031593979655\n",
      "cat_lgb_rf AUC: 0.9463146039921696\n",
      "cat_xgb_rf AUC: 0.9367980007060107\n",
      "bag_cat_xgb_gbc_lgb AUC: 0.9545651014088123\n"
     ]
    }
   ],
   "source": [
    "# get the predictions of the ensemble models\n",
    "\n",
    "# condense variables\n",
    "cat_pred = grid_cat_model.predict_proba(X)[:, 1]\n",
    "lgb_pred = grid_lgb_model.predict_proba(X)[:, 1]\n",
    "xgb_pred = grid_xgb_model.predict_proba(X)[:, 1]\n",
    "gbc_pred = grid_gbc_model.predict_proba(X)[:, 1]\n",
    "adb_pred = adb_model.predict_proba(X)[:, 1]\n",
    "rf_pred = grid_rf_model.predict_proba(X)[:, 1]\n",
    "\n",
    "# regular boosted models\n",
    "cat_lgb_pred = (cat_pred + lgb_pred) / 2\n",
    "cat_gbc_xgb_adb_pred = (cat_pred + gbc_pred + xgb_pred + adb_pred) / 4\n",
    "cat_lgb_gbc_xgb_pred = (cat_pred + lgb_pred + gbc_pred + xgb_pred) / 4\n",
    "cat_lgb_gbc_xgb_adb_pred = (cat_pred + lgb_pred + gbc_pred + xgb_pred + adb_pred) / 5\n",
    "cat_xgb_lgb_gbc_rf_pred = (cat_pred + xgb_pred + lgb_pred + gbc_pred + rf_pred) / 5\n",
    "cat_lgb_rf_pred = (cat_pred + lgb_pred + rf_pred) / 3\n",
    "cat_xgb_rf_pred = (cat_pred + xgb_pred + rf_pred) / 3\n",
    "\n",
    "# bagged models\n",
    "bag_cat_xgb_gbc_lgb_pred = (cat_bag.predict_proba(X)[:, 1] + xgb_bag.predict_proba(X)[:, 1] + gbc_bag.predict_proba(X)[:, 1] + lgb_bag.predict_proba(X)[:, 1]) / 4\n",
    "\n",
    "# get the AUC scores\n",
    "cat_lgb = roc_auc_score(y, cat_lgb_pred)\n",
    "cat_gbc_xgb_adb = roc_auc_score(y, cat_gbc_xgb_adb_pred)\n",
    "cat_lgb_gbc_xgb = roc_auc_score(y, cat_lgb_gbc_xgb_pred)\n",
    "cat_lgb_gbc_xgb_adb = roc_auc_score(y, cat_lgb_gbc_xgb_adb_pred)\n",
    "cat_xgb_lgb_gbc_rf = roc_auc_score(y, cat_xgb_lgb_gbc_rf_pred)\n",
    "cat_lgb_rf = roc_auc_score(y, cat_lgb_rf_pred)\n",
    "cat_xgb_rf = roc_auc_score(y, cat_xgb_rf_pred)\n",
    "\n",
    "# bagged models\n",
    "bag_cat_xgb_gbc_lgb = roc_auc_score(y, bag_cat_xgb_gbc_lgb_pred)\n",
    "\n",
    "# display the AUC scores\n",
    "print(f\"cat_lgb AUC: {cat_lgb}\")\n",
    "print(f\"cat_gbc_xgb_adb AUC: {cat_gbc_xgb_adb}\")\n",
    "print(f\"cat_lgb_gbc_xgb AUC: {cat_lgb_gbc_xgb}\")\n",
    "print(f\"cat_lgb_gbc_xgb_adb AUC: {cat_lgb_gbc_xgb_adb}\")\n",
    "print(f\"cat_xgb_lgb_gbc_rf AUC: {cat_xgb_lgb_gbc_rf}\")\n",
    "print(f\"cat_lgb_rf AUC: {cat_lgb_rf}\")\n",
    "print(f\"cat_xgb_rf AUC: {cat_xgb_rf}\")\n",
    "\n",
    "# bagged models\n",
    "print(f\"bag_cat_xgb_gbc_lgb AUC: {bag_cat_xgb_gbc_lgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ec300",
   "metadata": {},
   "source": [
    "## Training on the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c31af3",
   "metadata": {},
   "source": [
    "I took the best performing model from all the models that were tested above and converted the final `y_pred` series, then to a dataframe, then finally to a csv file for submission on Kaggle. I printed the length of the dataframe to ensure the file was formatted correct for Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a86b950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3854\n"
     ]
    }
   ],
   "source": [
    "y_pred = (grid_cat_model.predict_proba(test)[:, 1] + grid_lgb_model.predict_proba(test)[:, 1] + grid_gbc_model.predict_proba(test)[:, 1] + grid_xgb_model.predict_proba(test)[:, 1]) / 4\n",
    "y_pred = pd.DataFrame(y_pred, columns=['Y'], index=range(2854, 2854+len(y_pred)))\n",
    "y_pred.index.name = 'Id'\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fce1f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352810b7",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde19a74",
   "metadata": {},
   "source": [
    "To conclude, I began with preprocessing the data by looking at standard statistical measures for the columns of the dataset. Then, I continued with preprocessing by removing missing values and noisy columns. I attempted to normalize the data and remove outliers; however, these ideas were not beneficial to the predictive power of my models. \n",
    "\n",
    "Then I moved to creating my models, begininning with basic models, moving to boosted models, then concluding with a neural network. To select the best parameters for my models, I used `GridSearchCV`, and models that had GridSearch ran on th,e are denoted with the prefix `grid_`. Out of all these models, the LightGBM model was the singular most predictive model. When generating predictions, I used the soft probability values for each record being classified as a 1. I used the AUC score to compare the predictive power of the models. \n",
    "\n",
    "After creating my initial models, I tried various models to improve my models. I began with feature selection, trying forward and backward selection. These methods were computationally expensive, and I was surprised that neither improved the predictive capability of my models. Next, I attempted to ensemble the boosted models I created earlier using stacking and bagging. The stacking classifier and bagged models were better than my initial predictions using the normalized data and better than the boosted models that were not bagged. However, these ensembling methods did not generate the best predictions I got. By averaging the final predictions of my boosted models and bagged boosted models, I was able to generate my best predictions. I scored .908 accuracy on the private leaderboard, placing me at third, even though my public leaderboard score did not place in the top 20.\n",
    "\n",
    "Ultimately, my best model was the average predictions of the LightGBM, GradientBoosting, CatBoost, and XGBoost models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
